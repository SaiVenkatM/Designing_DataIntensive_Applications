{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb834576-5d07-4c3b-8ebc-922f7f6a145a",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "1. ***Reliability***: Tolerating hardware and software faults, human errors.\n",
    "   - For software typical expectation include:\n",
    "   - The application performs the function that user expected.\n",
    "   - It can tolerate the user maling mistakes or using software in unexpected ways.\n",
    "   - It performance is goog enough for required use case, under the expected load and data volume.\n",
    "   - The system prevents any unauthorized access and abuse.\n",
    "   - ***Fault-tolerant/resilient***: The things that can go wrong are called faults, and systems that anticipate faults and can cope with them are called fault-tolerant or resilient. \n",
    "2. ***Scalability***: Measuring load & performance, Latency percentiles, throughput.\n",
    "   - Scalability is the term we use to describe a system’s ability to cope with increased load.\n",
    "   - ***Latency***: The duration that a request is waiting to be handled—during which it is latent, awaiting service.\n",
    "   - ***Response Time***: Is what the client sees - besides the actual time to process the request (the service time), it includes network delays and queueing delays.\n",
    "   - ***Tail latencies***: High percentiles of response times, also known as , are important because they directly affect users’ experience of the service.\n",
    "   - ***Tail Latency Amplification***:\n",
    "3. ***Maintanability***: Operability, simplicity & evolvability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884da45-0554-41fc-932c-0bbafcb68206",
   "metadata": {},
   "source": [
    "### Reliability\n",
    "##### Hardware Faults\n",
    "- *Hard disks crash, RAM becomes faulty, the power grid has a blackout, someone unplugs the wrong network cable.*\n",
    "- *Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50 years. Thus, on a storage cluster with 10,000 disks, we should expect on average one disk to die per day*\n",
    "- *Add redundancy to the individual hardware compo‐ nents in order to reduce the failure rate of the system(multi-machine redundancy was only required by a small number of applications for which high availability was absolutely essential.)*\n",
    "- *However, as data volumes and applications’ computing demands have increased, more applications have begun using larger numbers of machines, which proportionally increases the rate of hardware faults.*\n",
    "- *There is a move toward systems that can tolerate the loss of entire machines, by using software fault-tolerance techniques in preference or in addition to hardware redundancy.(operational advantages: a single-server system requires planned downtime if you need to reboot the machine (to apply operating system security patches, for example), whereas a system that can tolerate machine failure can be patched one node at a time, without downtime of the entire system)*\n",
    "\n",
    "##### Software Errors\n",
    "- *Another class of fault is a systematic error within the system. Such faults are harder to anticipate, and because they are correlated across nodes, they tend to cause many more system failures than uncorrelated hardware faults*\n",
    "- *LeapSecond crash on June 30th 2012*\n",
    "- *A service that the system depends on that slows down, becomes unresponsive, or starts returning corrupted responses*\n",
    "- *Cascading failures, where a small fault in one component triggers a fault in another component, which in turn triggers further faults*\n",
    "\n",
    "##### Human Errors\n",
    "- **Even when they have the best intentions, humans are known to be unreliable.**\n",
    "- *Design systems in a way that minimizes opportunities for error*\n",
    "  1. For example, well-designed abstractions, APIs, and admin interfaces make it easy to do “the right thing” and discourage “the wrong thing.”\n",
    "  2. However, if the interfaces are too restrictive people will work around them, negating their benefit, so this is a tricky balance to get right.\n",
    "- *Decouple the places where people make the most mistakes from the places where they can cause failures.*\n",
    "  1. Provide fully featured non-production sandbox environments where people can explore and experiment safely\n",
    "- *Test thoroughly at all levels*\n",
    "- *Allow quick and easy recovery from human error*\n",
    "- 1. roll back configuration changes\n",
    "  2. roll out new code gradually (so that any unexpected bugs affect only a small subset of users).\n",
    "  3. And provide tools to recompute data (in case it turns out that the old computation was incorrect).\n",
    "- *Clear Monitoring.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3daeb07-fd53-4695-873f-f8468627572b",
   "metadata": {},
   "source": [
    "### Scalability\n",
    "Scalability means considering questions like 'if the system grows in particular way, what are our options for coping with the growth? and How?'\n",
    "\n",
    "##### Describing Load\n",
    "- *Load could be desribed with few numbers - Load Parameters*\n",
    "- *Requests per second to a  webserver, ratio of reads to writes in a DB, the number of simultaneously active users in chat room(Depends of architecture - Best choice of parameters)*\n",
    "- *Insert Twitter Usecase*\n",
    "\n",
    "\n",
    "##### Describing performance\n",
    "- **When you increase a load parameter and keep the system resources (CPU, mem‐ ory, network bandwidth, etc.) unchanged, how is the performance of your system affected?**\n",
    "- **When you increase a load parameter, how much do you need to increase the resources if you want to keep performance unchanged?**\n",
    "\n",
    "- *We need to think of response time not as a single number, but as a distribution of values that you can measure(In practice, in a system handling a variety of requests, the response time can vary a lot.)*\n",
    "- *Usually it is better to use percentiles(not mean mean is not a very good metric if you want to know your “typical” response time,). If you take your list of response times and sort it from fastest to slowest, then the median(50th Percentile) is the halfway point - for example, if your median response time is 200 ms, that means half your requests return in less than 200 ms, and half your requests take longer than that.*\n",
    "- *In order to figure out how bad your outliers are, you can look at higher percentiles: the 95th, 99th, and 99.9th percentiles are common*\n",
    "- *For example, Amazon describes response time requirements for internal services in terms of the 99.9th percentile, even though it only affects 1 in 1,000 requests. This is because the customers with the slowest requests are often those who have the most data on their accounts because they have made many purchases—that is, they’re the most valuable customers*\n",
    "- *Queueing delays often account for a large part of the response time at high percentiles*\n",
    "- *It is important to measure response times on the client side*\n",
    "- *If you want to add response time percentiles to the monitoring dashboards for your services, you need to efficiently calculate them on an ongoing basis. For example, you may want to keep a rolling window of response times of requests in the last 10 minutes. Every minute, you calculate the median and various percentiles over the val‐ ues in that window and plot those metrics on a graph.\n",
    "The naïve implementation is to keep a list of response times for all requests within the time window and to sort that list every minute. If that is too inefficient for you, there are algorithms that can calculate a good approximation of percentiles at minimal CPU and memory cost, such as forward decay, t-digest, or HdrHistogram. Beware that averaging percentiles, e.g., to reduce the time resolution or to combine data from several machines, is mathematically meaningless—the right way of aggregating response time data is to add the histograms.*\n",
    "  \n",
    "##### Approaches for Coping with Load\n",
    "- **How do we maintain good performance even when our load parameters increase by some amount?**\n",
    "- *People often talk of a dichotomy between scaling up (vertical scaling, moving to a more powerful machine) and scaling out (horizontal scaling, distributing the load across multiple smaller machines). Distributing load across multiple machines is also known as a shared-nothing architecture.*\n",
    "- *A system that can run on a single machine is often simpler, but high-end machines can become very expensive, so very intensive workloads often can’t avoid scaling out.*\n",
    "- *While distributing stateless services across multiple machines is fairly straightforward, taking stateful data systems from a single node to a distributed setup can introduce a lot of additional complexity. For this reason, common wisdom until recently was to keep your database on a single node (scale up) until scaling cost or high-availability requirements forced you to make it distributed.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1eadf-281a-43b2-a83e-68638c6615a8",
   "metadata": {},
   "source": [
    "### Maintainability\n",
    "##### Operability\n",
    "- *Make it easy for operations teams to keep the system running smoothly.*\n",
    "- *“good operations can often work around the limitations of bad (or incomplete) software, but good software cannot run reliably with bad operations”*\n",
    "- *Monitoring health of system, Tracking down the cause of problems, keeping software and platform upto date inlcuding security patches.*\n",
    "- *Defining process that make operations predictable and help keep the production environment stable.*\n",
    "- *Preserving the orgranization's knowledge about the system.*\n",
    "- *Good operability means making routine tasks easy, allowing the operations team to focus their efforts on high-value activities.*\n",
    "- *Providing visibilty into runtime behaviour and internals of the system*\n",
    "- *Provding good support for automation and integration*\n",
    "- *Avoding dependency on Individual machines*\n",
    "- *Providing good default behaviour, but also providing admins to overide the defaults*\n",
    "\n",
    "##### Simplicity\n",
    "- *A software project mired in complexity is sometimes described as a big ball of mud*\n",
    "- *symptoms of complexity: explosion of the state space, tight coupling of modules, tangled dependencies, inconsistent naming and terminology, hacks aimed at solving performance problems, special-casing to work around issues elsewhere*\n",
    "- *Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity. Moseley and Marks define complex‐ ity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.*\n",
    "- *One of the best tools we have for removing accidental complexity is abstraction. A good abstraction can hide a great deal of implementation detail behind a clean, simple-to-understand façade.*\n",
    "- *Keep our eyes open for good abstractions that allow us to extract parts of a large system into well-defined, reusable components.*\n",
    "\n",
    "##### Evolvability\n",
    "- *In terms of organizational processes, Agile working patterns provide a framework for adapting to change*\n",
    "- *Test-driven-development and refactoring patterns could be helpful that involves a frequent changing environment*\n",
    "- *For example, how would you “refactor” Twitter’s architecture for assembling home timelines (Refer to insert twitter usecase from describing load) from approach 1 to approach 2?*\n",
    "- *The ease with which you can modify a data system, and adapt it to changing require‐ ments, is closely linked to its simplicity and its abstractions*\n",
    "- *A different word to refer to agility on a data sys‐ tem level: evolvability*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7e840-a726-4ad1-99b4-05ff7aa22e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34162afa-5e18-4b53-895c-abe45859c113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
